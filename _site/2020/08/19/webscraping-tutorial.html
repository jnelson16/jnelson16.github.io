<!DOCTYPE html>

<html>

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>
    Tutorial: Building a Web Scraper in Python - Jonathan Nelson
    
  </title>

  <meta name="description" content="In the previous post, we discussed some of the basics of web scraping using Python. Here, we will actually build a full web scraper using the tools from the ...">

  <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="/assets/vendor/bootstrap/css/bootstrap.min.css">

  <link rel="stylesheet" href="/assets/vendor/fontawesome-free/css/all.min.css">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/2020/08/19/webscraping-tutorial.html">
  <link rel="alternate" type="application/rss+xml" title="Jonathan Nelson" href="/feed.xml">

</head>


<body>

  
  <!-- Navigation -->
<nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand" href="/">Jonathan Nelson</a>
    <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
      <i class="fa fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item">
          <a class="nav-link" href="/blog">Blog</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/resume">Resume</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/contact">Contact</a>
        </li>
      </ul>
    </div>
  </div>
</nav>

  

  <style>
img {
  max-width: 100%;
  height: auto;
  border: 1px solid #ccc;
}
div.highlight {
  background-color: #f2f2f2;
}
code {
  color: black;
}
</style>

<!-- Page Header -->

<header class="masthead" style="background-image: url('/img/blog/post 2.jpg')">
  
    <div class="overlay"></div>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <div class="post-heading">
            <h1>Tutorial: Building a Web Scraper in Python</h1>
            
            <span class="meta">
              August 19, 2020 &middot; <span class="reading-time" title="Estimated read time">
  
   9 mins  read </span>

            </span>
          </div>
        </div>
      </div>
    </div>
  </header>

  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">

        <p>In the <a href="/2020/08/12/webscraping.html">previous post</a>, we discussed some of the basics of web scraping using Python. Here, we will actually build a full web scraper using the tools from the previous post.</p>

<p><em>Author’s note: I put together a Jupyter notebook to more interactively follow along with this tutorial. You can find that in my GitHub repository <a href="https://github.com/jnelson16/tutorials/tree/master/notebooks">here</a>. In addition, I wrote a full python script <a href="https://github.com/jnelson16/tutorials/tree/master/scripts">here</a>, with more functions and a command line interface you can play around with.</em></p>

<h3 id="determining-what-to-scrape">Determining what to scrape</h3>

<p>For the sake of simplicity (and the opportunity to be meta), this tutorial will scrape the very website where this post lives. We will start at the homepage, and navigate through the links, scraping the text and data as we go, ending by putting the data in to a pandas dataframe.</p>

<p><em>N.B.: As this website evolves, some parts of this post may be less accurate, though I will attempt to update it as the website changes.</em></p>

<p>Let’s begin by simply going to the homepage of jonathannelson.io in a browser to see what the page looks like. Below is a screenshot of what you should see:</p>

<p><img src="/img/blog/homepage.png" alt="homepage" /></p>

<p>We can play around with the website to get an idea of what is scrapable and what links we can use to traverse the website. At the top, we see three links that lead to the blog, the resume, and the contact form. In the middle we see a button with a link that will also take us to the resume. In the footer of the page, we see the three social media links we already scraped in the previous post. For purposes of scraping the website, the links at the top seem to be the most useful, so let’s look more into those.</p>

<h3 id="using-inspect">Using inspect</h3>

<p>In order to scrape the website, we will need to know a bit about the HTML of the site. If you are using Google Chrome (why you would use anything else? (don’t @ me Firefox users)), you can utilize the inspect feature to look at the HTML of the site. Right click on the “blog” link at the top and left click inspect. You should now see something like the screenshot below:</p>

<p><img src="/img/blog/inspect.png" alt="inspect" /></p>

<p>We can see here that each of the navigation links are contained in an <code class="highlighter-rouge">&lt;li&gt;</code> element with a class attribute “nav-item” and the link itself is an <code class="highlighter-rouge">&lt;a&gt;</code> element with class attribute “nav-link.” For purposes of scraping, either will do, but let’s go with the <code class="highlighter-rouge">&lt;li&gt;</code> elements.</p>

<h3 id="parsing-with-requests-and-beautifulsoup">Parsing with requests and BeautifulSoup</h3>

<p>Now that we know what we want to scrape and what the HTML looks like, we can finally start coding. Borrowing code from the previous post, we can get the raw HTML of the homepage and convert it into a BeautifulSoup object:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import bs4
import requests
url = 'http://jonathannelson.io'
content = requests.get(url).content
soup = bs4.BeautifulSoup(content, 'lxml')
</code></pre></div></div>

<p>To make things easier, let’s put this code into a function so we don’t have to type this all out every time we want to get a soup object from a URL:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def get_soup(url):
    return bs4.BeautifulSoup(requests.get(url).content, 'lxml')
</code></pre></div></div>

<p>Then we can get the soup object simply by typing <code class="highlighter-rouge">soup = get_soup(url)</code>.</p>

<h3 id="navigating-to-other-pages">Navigating to other pages</h3>

<p>Now that we have the soup object, we can parse the HTML using the element tags we discussed above. We can utilize the class attribute “nav-item” to iterate only through <code class="highlighter-rouge">&lt;li&gt;</code> elements we want. The following code will get us the three links to get to the rest of the website, and then get soup objects of each of the three pages:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import urllib
for page in soup.find_all('li', class_="nav-item"):
    page_url = urllib.parse.urljoin(url, page.find('a')['href'])
    page_soup = get_soup(page_url)
</code></pre></div></div>

<p>The URL for each page comes from “href” attribute of the <code class="highlighter-rouge">&lt;a&gt;</code> element within the <code class="highlighter-rouge">&lt;li&gt;</code> element. But the URLs here only include the name of the page, not the full address. We can use the <strong>urllib</strong> library to join the page URLs with the original URL from above to get the full web address, and pass that new URL into the <code class="highlighter-rouge">get_soup()</code> function. Because this function both uses a get request and parses the HTML into a soup object, it is almost like we are clicking on the link and loading that new page.</p>

<h3 id="scraping-text-and-data-from-a-page">Scraping text and data from a page</h3>

<p>The first page we visit using the scraper is the blog page. In order to navigate through this page, we should inspect it just like we did for the homepage. You should see something like this:</p>

<p><img src="/img/blog/inspect-blog.png" alt="inspect the blog" /></p>

<p>We can see that each blog post is represented in the HTML as an <code class="highlighter-rouge">&lt;article&gt;</code> element with a class attribute “post-preview.” We can use this element to get both the links for each blog post as well as some metadata associated with them (which will eventually be added to the empty list called <code class="highlighter-rouge">data</code>):</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pandas as pd
data = []
for post in page_soup.find_all(class_="post-preview"):
    post_title = post.find(class_="post-title").text
    post_date = pd.to_datetime(
        post.find(class_="post-meta").text.split(' · ')[0]).date()
    post_url = urllib.parse.urljoin(url, post.find('a')['href'])
    post_soup = get_soup(post_url)
</code></pre></div></div>

<p>Here we have collected not only the URL for each blog post, but the title and date of the posts as well. <em>Note that we use the pandas function <code class="highlighter-rouge">to_datetime().date()</code> to get the date in a clean format.</em> We can eventually put these metadata into a pandas dataframe, but first let’s collect the actual text from the page of the posts.</p>

<p>The main content of each blog post is contained in the third <code class="highlighter-rouge">div</code> element with a class attribute “container” on the page, so we can get the text using:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>post_text = post_soup.find_all('div', class_="container")[2].get_text('\n')
</code></pre></div></div>

<p>For full text like this, it is helpful to use <code class="highlighter-rouge">.get_text('\n')</code> instead of <code class="highlighter-rouge">.text</code> just in case the text elements run into each other. Now, let’s write this text to file so we don’t lose it (using tools from the <strong>pathlib</strong> library):</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from pathlib import Path
outpath = Path('blogs').joinpath(
    post_date, post_title).with_suffix('.txt')
# Create directory structure if it does not exist
if not outpath.parent.exists():
    outpath.parent.mkdir(parents=True)
# Write text to file
outpath.write_text(post_text)
</code></pre></div></div>

<p>This will write the text into a folder called “blogs,” further organized by date and the title of the post. The text from these posts could then be analyzed using natural language processing (see future posts for that).</p>

<h3 id="putting-data-into-pandas">Putting data into pandas</h3>

<p>We now have both the text from the blogs and metadata associated with them. Let’s put that metadata into a dataframe. In the for-loop for each post we can append each post’s metadata to the list created above:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>data.append((post_date, post_title, post_url, outpath))
</code></pre></div></div>

<p>Then, outside of the for-loop, we can put this data into a pandas dataframe, and write this data into a csv:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>df = pd.DataFrame(
    data=data,
    columns=['date', 'title', 'url', 'filepath'])
df.to_csv('blog_metadata.csv', index=False)
</code></pre></div></div>

<p>Since we have the filepath, we could use this csv later as metadata for a natural language processing project using the blog text.</p>

<h3 id="summary">Summary</h3>

<p>Putting it all together, we get code that should look something like this (not including the imports and function definition):</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>url = 'http://jonathannelson.io'
soup = get_soup(url)
data = []
for link in soup.find_all('li', class_="nav-item"):
    page_url = urllib.parse.urljoin(url, link.find('a')['href'])
    page_soup = get_soup(page_url)
    for post in page_soup.find_all(class_="post-preview"):
        post_title = post.find(class_="post-title").text
        post_date = pd.to_datetime(
            post.find(class_="post-meta").text.split(' · ')[0]).date()
        post_url = urllib.parse.urljoin(url, post.find('a')['href'])
        post_soup = get_soup(post_url)
        post_text = post_soup.find_all(
            'div', class_="container")[2].get_text('\n')
        outpath = Path('blogs').joinpath(
            str(post_date), post_title).with_suffix('.txt')
        if not outpath.parent.exists():
            outpath.parent.mkdir(parents=True)
        outpath.write_text(post_text)
        data.append((
            post_date, post_title, post_url, outpath
        ))
df = pd.DataFrame(data, columns=['date', 'title', 'url', 'filepath'])
df.to_csv('blog_metadata.csv')
</code></pre></div></div>

<p>Unfortunately, we only had time to scrape the first of the three pages on this website in this post, but try to scrape data or text from the other two on your own. Feel free to <a href="/contact">contact me</a> or reach out on social media (links below) for questions or other feedback!</p>


        <hr>

        <div class="clearfix">

          
          <a class="btn btn-secondary float-left" href="/2020/08/12/webscraping.html" data-toggle="tooltip" data-placement="top" title="Introduction to Web Scraping for Data Science Using Python">&larr; Previous<span class="d-none d-md-inline">
              Post</span></a>
          
          
          <a class="btn btn-secondary float-right" href="/2021/07/21/statistics.html" data-toggle="tooltip" data-placement="top" title="Statistical Fallacies in the Ohio Motorcycle Manual">Next<span class="d-none d-md-inline">
              Post</span> &rarr;</a>
          

        </div>

      </div>
    </div>
  </div>

<!-- External links open new tabs -->
<script>
  const links = document.links;
  for (var i = 0; i < links.length; i++) {
    if (window.location.hostname != links[i].hostname) {
      links[i].target = "_blank";
    } 
  }
</script>


  <!-- Footer -->

<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">
        <ul class="list-inline text-center">
          
          <li class="list-inline-item">
            <a href="https://github.com/jnelson16" target="_blank">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fab fa-github fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
          
          
          <li class="list-inline-item">
            <a href="https://twitter.com/NelsonEcon16" target="_blank">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
          
          <li class="list-inline-item">
            <a href="https://www.linkedin.com/in/jonathan-nelson-3056a462" target="_blank">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
        </ul>
        <p class="copyright text-muted">Copyright &copy; 2021 Jonathan Nelson</p>
      </div>
    </div>
  </div>
</footer>


  <script src="/assets/vendor/jquery/jquery.min.js"></script>
<script src="/assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
<script src="/assets/vendor/startbootstrap-clean-blog/js/clean-blog.min.js"></script>

<script src="/assets/scripts.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-175200606-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-175200606-1');
</script>



</body>

</html>
